import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
from transformers import CLIPModel, CLIPTokenizer
from motion_loader import get_dataset_loader  
from tqdm import tqdm
import yaml
from argparse import Namespace
from options.get_opt import get_opt
import os

GLOBAL_CACHE = {
    "clip_model": None,
    "clip_tokenizer": None,
    "motion_encoder": None,
    "clip_motion_align_model": None,
    "device": None
}


# ---------------------------
# è®¾ç½®/è·å– å…¨å±€ device
# ---------------------------
def set_global_device(dev):
    """
    è®¾ç½®å…¨å±€ deviceï¼ˆä¾‹å¦‚ 'cuda:0' æˆ– 'cpu'ï¼‰
    """
    GLOBAL_CACHE["device"] = dev


def get_global_device():
    """
    è·å–å…¨å±€ deviceï¼Œå¦‚æœªè®¾ç½®åˆ™é»˜è®¤ä½¿ç”¨ 'cuda' or 'cpu'
    """
    if GLOBAL_CACHE["device"] is None:
        dev = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        GLOBAL_CACHE["device"] = dev
    return GLOBAL_CACHE["device"]


# ---------------------------
# PositionalEncoding å®šä¹‰
# ---------------------------
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000, dropout=0.2):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))
        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ä¸‹æ ‡
        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ä¸‹æ ‡
        pe = pe.unsqueeze(1)  # (max_len, 1, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        x: (T, B, D)
        """
        seq_len = x.size(0)
        x = x + self.pe[:seq_len, :]
        return self.dropout(x)


# ---------------------------
# MotionEncoder å®šä¹‰
# ---------------------------
class MotionEncoder(nn.Module):
    def __init__(self, input_dim=263, embed_dim=512, num_heads=8, num_layers=4,
                 dim_feedforward=2048, dropout=0.2, max_seq_length=196):
        super(MotionEncoder, self).__init__()
        self.embed_dim = embed_dim
        self.input_proj = nn.Linear(input_dim, embed_dim)
        self.pos_encoder = PositionalEncoding(d_model=embed_dim, max_len=max_seq_length, dropout=dropout)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=False
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, motion, lengths):
        """
        motion: (B, T, D)
        lengths: (B,)
        """
        x = self.input_proj(motion).transpose(0, 1)  # (T, B, embed_dim)
        x = self.pos_encoder(x)                      # (T, B, embed_dim)

        B, T = motion.size(0), motion.size(1)
        device = motion.device
        pad_mask = torch.zeros((B, T), dtype=torch.bool, device=device)
        for i, length in enumerate(lengths):
            if length < T:
                pad_mask[i, length:] = True

        x = self.transformer_encoder(x, src_key_padding_mask=pad_mask)  # (T, B, embed_dim)
        x = x.transpose(0, 1)  # (B, T, embed_dim)

        pooled_list = []
        for i in range(B):
            valid_len = lengths[i]
            if valid_len > 0:
                pooled_list.append(x[i, :valid_len].mean(dim=0))
            else:
                pooled_list.append(torch.zeros(self.embed_dim, device=device))
        pooled = torch.stack(pooled_list, dim=0)  # (B, embed_dim)
        pooled = self.dropout(pooled)
        pooled = self.fc(pooled)  # (B, embed_dim)
        return pooled


# ---------------------------
# ClipMotionAlignModel å®šä¹‰
# ---------------------------
class ClipMotionAlignModel(nn.Module):
    def __init__(self, clip_model: CLIPModel, motion_encoder: nn.Module, temperature=0.07):
        super().__init__()
        self.clip_model = clip_model
        self.motion_encoder = motion_encoder
        # åˆå§‹åŒ– logit_scale = log(1/temperature)
        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1.0 / temperature)))

    def forward(self, motion, lengths, input_ids, attention_mask):
        motion_emb = self.motion_encoder(motion, lengths)
        text_emb = self.clip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)
        return motion_emb, text_emb


def _init_clip_motion_model(model_path):
    """
    ä»…åœ¨å…¨å±€ç¼“å­˜ä¸ºç©ºæ—¶ï¼Œåˆå§‹åŒ– CLIPæ¨¡å‹ã€tokenizerã€MotionEncoder å¹¶åŠ è½½é¢„è®­ç»ƒæƒé‡ã€‚
    ä¹‹åå­˜å…¥ GLOBAL_CACHEï¼Œä¾›åç»­é‡å¤ä½¿ç”¨ã€‚
    """
    if GLOBAL_CACHE["clip_motion_align_model"] is not None:
        # å·²ç»åŠ è½½è¿‡ï¼Œç›´æ¥è¿”å›
        return

    # æ ¹æ®åŸå§‹è®­ç»ƒçš„é…ç½®å®šä¹‰
    class OPT:
        embed_dim = 768
        # æ­¤å¤„ä»å…¨å±€è·å– device
        device = get_global_device()
        clip_model_name = "openai/clip-vit-large-patch14"
        max_length = 77
        input_dim = 263  # æ”¹ä¸º263ï¼Œå¯¹åº”t2mæ•°æ®é›†
        max_seq_length = 196

    # åŠ è½½ CLIP æ¨¡å‹å’Œ tokenizer
    clip_model = CLIPModel.from_pretrained(OPT.clip_model_name)
    clip_tokenizer = CLIPTokenizer.from_pretrained(OPT.clip_model_name)
    clip_model.to(OPT.device)

    # æ„å»º MotionEncoder ä¸æ•´ä½“æ¨¡å‹
    motion_encoder = MotionEncoder(
        input_dim=OPT.input_dim,
        embed_dim=OPT.embed_dim,
        num_heads=8,
        num_layers=4,
        dim_feedforward=2048,
        dropout=0.2,
        max_seq_length=OPT.max_seq_length
    )

    model = ClipMotionAlignModel(
        clip_model=clip_model,
        motion_encoder=motion_encoder,
        temperature=0.07
    ).to(OPT.device)

    # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œæ”¯æŒæ–°çš„checkpointæ ¼å¼
    try:
        checkpoint = torch.load(model_path, map_location=OPT.device)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°çš„è®­ç»ƒcheckpointæ ¼å¼ (train_moclip.pyä¿å­˜çš„æ ¼å¼)
        if 'model_state_dict' in checkpoint:
            # æ–°è®­ç»ƒçš„checkpointæ ¼å¼
            state_dict = checkpoint['model_state_dict']
            epoch = checkpoint.get('epoch', 'unknown')
            best_r3 = checkpoint.get('best_r3', 'unknown')
            print(f"æˆåŠŸåŠ è½½æ–°è®­ç»ƒçš„checkpointæ ¼å¼æ¨¡å‹")
            print(f"  è®­ç»ƒè½®æ•°: {epoch}")
            print(f"  æœ€ä½³R@3: {best_r3}")
            print(f"  æ¨¡å‹è·¯å¾„: {model_path}")
        else:
            # åŸå§‹çš„state_dictæ ¼å¼
            state_dict = checkpoint
            print("åŠ è½½åŸå§‹state_dictæ ¼å¼æ¨¡å‹")
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€
        model.load_state_dict(state_dict, strict=True)
        print("æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        
    except Exception as e:
        print(f"åŠ è½½æ¨¡å‹æƒé‡å¤±è´¥: {e}")
        print("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡")
    
    model.eval()

    # ç¼“å­˜åˆ°å…¨å±€å˜é‡
    GLOBAL_CACHE["clip_model"] = clip_model
    GLOBAL_CACHE["clip_tokenizer"] = clip_tokenizer
    GLOBAL_CACHE["motion_encoder"] = motion_encoder
    GLOBAL_CACHE["clip_motion_align_model"] = model


# ---------------------------
# å®šä¹‰è·å–æ–‡æœ¬ä¸åŠ¨ä½œç¼–ç çš„å‡½æ•°ï¼ˆä¿æŒåŸæ¥å£ï¼‰
# ---------------------------
def get_co_embeddings_2(captions, motions, model_path="/data/wenshuo/project/jhz/StableMoFusion/data/clip_motion_align_epoch_21.pt"):
    """
    å‚æ•°ï¼š
        captions: List[str]ï¼Œæ–‡æœ¬æè¿°åˆ—è¡¨
        motions: list æˆ– tensorï¼ŒåŠ¨ä½œæ•°æ®ï¼Œå½¢çŠ¶åº”ä¸º (B, T, input_dim) æˆ–è€… listï¼Œæ¯ä¸ªå…ƒç´ ä¸º (T, input_dim) çš„æ•°ç»„
        model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
    è¿”å›ï¼š
        text_embeddings, motion_embeddings
    """
    # å¦‚æœå…¨å±€æ¨¡å‹è¿˜æœªåˆå§‹åŒ–ï¼Œåˆ™è¿›è¡Œåˆå§‹åŒ–ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
    _init_clip_motion_model(model_path)

    # å–å‡ºå·²ç¼“å­˜çš„æ¨¡å‹ã€tokenizerã€device
    clip_motion_model = GLOBAL_CACHE["clip_motion_align_model"]
    clip_tokenizer = GLOBAL_CACHE["clip_tokenizer"]
    device = get_global_device()

    # ---------------------------
    # æ–‡æœ¬å¤„ç†
    # ---------------------------
    captions_lower = [caption.lower() for caption in captions]
    text_encodings = clip_tokenizer(
        captions_lower,
        padding=True,
        truncation=True,
        max_length=77,  # ä¸åŸ OPT.max_length ä¿æŒä¸€è‡´
        return_tensors="pt"
    )
    input_ids = text_encodings["input_ids"].to(device)
    attention_mask = text_encodings["attention_mask"].to(device)

    # ---------------------------
    # åŠ¨ä½œæ•°æ®å¤„ç†
    # ---------------------------
    if isinstance(motions, list):
        motion_tensors = []
        lengths = []
        for m in motions:
            m_tensor = torch.tensor(m, dtype=torch.float32)
            motion_tensors.append(m_tensor)
            lengths.append(m_tensor.shape[0])

        max_T = max(lengths)
        padded_motions = []
        for m_tensor in motion_tensors:
            T = m_tensor.shape[0]
            if T < max_T:
                pad = torch.zeros((max_T - T, m_tensor.shape[1]), dtype=torch.float32)
                m_tensor = torch.cat([m_tensor, pad], dim=0)
            padded_motions.append(m_tensor)
        motions_tensor = torch.stack(padded_motions, dim=0)
        lengths_tensor = torch.tensor(lengths, dtype=torch.long, device=device)
    else:
        motions_tensor = motions.float().to(device)
        B, T, _ = motions_tensor.shape
        lengths_tensor = torch.tensor([T] * B, dtype=torch.long, device=device)

    # ---------------------------
    # æ¨¡å‹å‰å‘ä¼ æ’­ï¼Œè·å¾—ç¼–ç 
    # ---------------------------
    with torch.no_grad():
        motion_emb, text_emb = clip_motion_model(motions_tensor, lengths_tensor, input_ids, attention_mask)

    # å¯¹ç¼–ç è¿›è¡Œå½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼‰
    motion_embeddings = F.normalize(motion_emb, dim=-1).cpu()
    text_embeddings = F.normalize(text_emb, dim=-1).cpu()

    return text_embeddings, motion_embeddings


def encode_dataset_motions(opt_path=None, dataset_name='t2m', model_path="/data/wenshuo/project/jhz/StableMoFusion/data/clip_motion_align_epoch_21.pt", 
                          output_path="dataset_motion_embeddings.npy", batch_size=32):
    """
    ç¼–ç æ•°æ®é›†ä¸­çš„æ‰€æœ‰motionå¹¶ä¿å­˜åˆ°npyæ–‡ä»¶
    
    å‚æ•°ï¼š
        opt_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        dataset_name: æ•°æ®é›†åç§° ('t2m' æˆ– 'kit')
        model_path: CLIP motion alignæ¨¡å‹æƒé‡è·¯å¾„
        output_path: è¾“å‡ºnpyæ–‡ä»¶è·¯å¾„
        batch_size: æ‰¹å¤„ç†å¤§å°
    """
    
    # è®¾ç½®device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    set_global_device(device)
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆå§‹åŒ–å‚æ•°
    opt = Namespace()
    if opt_path and os.path.exists(opt_path):
        get_opt(opt, opt_path)
    else:
        # ä½¿ç”¨é»˜è®¤é…ç½®
        opt.dataset_name = dataset_name
        opt.batch_size = batch_size
        opt.device = device
        opt.max_length = 77
        opt.feat_bias = 5  # æ·»åŠ feat_biaså‚æ•°
        opt.max_text_len = 20  # æ·»åŠ max_text_lenå‚æ•°
        opt.unit_length = 4  # æ·»åŠ unit_lengthå‚æ•°
        
        if dataset_name == 't2m':
            opt.joints_num = 22
            opt.dim_pose = 263
            opt.max_motion_length = 196
            opt.radius = 4
            opt.fps = 20
            opt.data_root = './dataset/HumanML3D'
            opt.motion_dir = os.path.join(opt.data_root, 'new_joint_vecs')
            opt.text_dir = os.path.join(opt.data_root, 'texts')
            opt.mean_path = os.path.join(opt.data_root, 'Mean.npy')
            opt.std_path = os.path.join(opt.data_root, 'Std.npy')
            opt.split_dir = os.path.join(opt.data_root, 'train_val.txt')
            # æ·»åŠ metaç›®å½•è®¾ç½®
            opt.meta_dir = './checkpoints/t2m/clip/meta'
            opt.eval_meta_dir = './dataset'  # åŒ…å«t2m_mean.npyå’Œt2m_std.npyçš„ç›®å½•
            opt.glove_dir = './dataset'  # è®¾ç½®gloveç›®å½•ï¼Œè™½ç„¶evalæ¨¡å¼ä¸éœ€è¦
        elif dataset_name == 'kit':
            opt.joints_num = 21
            opt.dim_pose = 251
            opt.max_motion_length = 196
            opt.radius = 240 * 8
            opt.fps = 12.5
            opt.data_root = './dataset/KIT-ML'
            opt.motion_dir = os.path.join(opt.data_root, 'new_joint_vecs')
            opt.text_dir = os.path.join(opt.data_root, 'texts')
            opt.mean_path = os.path.join(opt.data_root, 'Mean.npy')
            opt.std_path = os.path.join(opt.data_root, 'Std.npy')
            opt.split_dir = os.path.join(opt.data_root, 'train_val_test.txt')
            # æ·»åŠ metaç›®å½•è®¾ç½®
            opt.meta_dir = './checkpoints/kit/meta'
            opt.eval_meta_dir = './dataset'  # åŒ…å«kit_mean.npyå’Œkit_std.npyçš„ç›®å½•
            opt.glove_dir = './dataset'  # è®¾ç½®gloveç›®å½•
    
    print(f"æ•°æ®é›†: {opt.dataset_name}")
    print(f"æ‰¹å¤„ç†å¤§å°: {opt.batch_size}")
    
    # åˆå§‹åŒ–CLIPæ¨¡å‹
    _init_clip_motion_model(model_path)
    model = GLOBAL_CACHE["clip_motion_align_model"]
    tokenizer = GLOBAL_CACHE["clip_tokenizer"]
    
    # è·å–æ•°æ®åŠ è½½å™¨ - ä½¿ç”¨trainæ¨¡å¼ï¼Œé¿å…éœ€è¦GloVeè¯å‘é‡
    test_loader = get_dataset_loader(
        opt,
        batch_size=opt.batch_size,
        split='test',
        mode='train'  # æ”¹ä¸ºtrainæ¨¡å¼
    )
    
    print(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸï¼Œæ€»æ‰¹æ¬¡æ•°: {len(test_loader)}")
    
    # å­˜å‚¨æ‰€æœ‰motion embeddings
    motions = []
    
    print("å¼€å§‹ç¼–ç motionæ•°æ®...")
    for batch_idx, batch_data in enumerate(tqdm(test_loader, desc="ç¼–ç è¿›åº¦")):
        torch.cuda.empty_cache()  
        
        # trainæ¨¡å¼è¿”å›ç®€å•çš„3ä¸ªå…ƒç´ 
        caption, motion, m_length = batch_data
        
        # å¤„ç†æ–‡æœ¬
        caption = [c.lower() for c in caption]
        text_enc = tokenizer(
            caption,
            padding=True,
            truncation=True,
            max_length=opt.max_length,
            return_tensors="pt"
        )
        input_ids = text_enc["input_ids"].to(opt.device)
        attention_mask = text_enc["attention_mask"].to(opt.device)
        
        # å¤„ç†motionæ•°æ®
        if isinstance(motion, list):
            motion = torch.stack([torch.tensor(m, dtype=torch.float32) for m in motion], dim=0)
        else:
            motion = motion.float()
        motion = motion.to(opt.device)  
        m_length = m_length.to(opt.device)
        
        # å‰å‘ä¼ æ’­è·å–embeddings
        with torch.no_grad():
            motion_emb, text_emb = model(motion, m_length, input_ids, attention_mask)
        
        # ä¿å­˜motion embedding
        motions.append(motion_emb.cpu().numpy())  
        
        # æ¸…ç†ç¼“å­˜å¹¶åˆ é™¤ä¸å†éœ€è¦çš„å˜é‡
        del motion_emb
        del text_emb
        torch.cuda.empty_cache()
        
        if (batch_idx + 1) % 100 == 0:
            print(f"å·²å¤„ç† {batch_idx + 1}/{len(test_loader)} æ‰¹æ¬¡")
    
    # å°†æ‰€æœ‰çš„motion_embæ‹¼æ¥æˆä¸€ä¸ªnumpyæ•°ç»„
    motions_cat = np.concatenate(motions, axis=0)  # åœ¨ç¬¬ä¸€ä¸ªç»´åº¦æ‹¼æ¥ï¼Œå½¢æˆn*768
    
    print(f"æ‰€æœ‰motionç¼–ç å®Œæˆï¼Œæ€»æ•°æ®å½¢çŠ¶: {motions_cat.shape}")
    
    # ä¿å­˜åˆ°npyæ–‡ä»¶
    np.save(output_path, motions_cat)
    print(f"Motion embeddingså·²ä¿å­˜åˆ°: {output_path}")
    
    # æ¸…ç†motionsåˆ—è¡¨ï¼Œé‡Šæ”¾å†…å­˜
    del motions
    torch.cuda.empty_cache()
    
    return motions_cat


def encode_keyword_motions(keyword, opt_path=None, dataset_name='t2m', 
                          model_path="/data/wenshuo/project/jhz/StableMoFusion/data/clip_motion_align_epoch_21.pt", 
                          output_dir="./", batch_size=32, search_all_splits=True):
    """
    ç¼–ç æ•°æ®é›†ä¸­åŒ…å«ç‰¹å®šå…³é”®è¯çš„motionå’Œtextï¼Œå¹¶è®¡ç®—å®ƒä»¬çš„ä½™å¼¦ç›¸ä¼¼åº¦
    
    å‚æ•°ï¼š
        keyword: è¦æœç´¢çš„å…³é”®è¯
        opt_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        dataset_name: æ•°æ®é›†åç§° ('t2m' æˆ– 'kit')
        model_path: CLIP motion alignæ¨¡å‹æƒé‡è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        batch_size: æ‰¹å¤„ç†å¤§å°
        search_all_splits: æ˜¯å¦æœç´¢æ‰€æœ‰åˆ†å‰²ï¼ˆtrainã€testã€valï¼‰
    """
    
    # è®¾ç½®device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    set_global_device(device)
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"æœç´¢å…³é”®è¯: '{keyword}'")
    
    # åˆ›å»ºå…³é”®è¯å˜ä½“åˆ—è¡¨ï¼Œæ”¯æŒæ›´çµæ´»çš„åŒ¹é…
    keyword_variants = [
        keyword.lower(),
        keyword.lower() + 's',  # å¤æ•°å½¢å¼
        keyword.lower() + '-like',  # -likeåç¼€
        keyword.lower() + ' like',  # ç©ºæ ¼like
        'like a ' + keyword.lower(),  # like aå‰ç¼€
        'like ' + keyword.lower(),   # likeå‰ç¼€
    ]
    print(f"æœç´¢å˜ä½“: {keyword_variants}")
    
    # åˆå§‹åŒ–å‚æ•°
    opt = Namespace()
    if opt_path and os.path.exists(opt_path):
        get_opt(opt, opt_path)
    else:
        # ä½¿ç”¨é»˜è®¤é…ç½®
        opt.dataset_name = dataset_name
        opt.batch_size = batch_size
        opt.device = device
        opt.max_length = 77
        opt.feat_bias = 5
        opt.max_text_len = 20
        opt.unit_length = 4
        
        if dataset_name == 't2m':
            opt.joints_num = 22
            opt.dim_pose = 263
            opt.max_motion_length = 196
            opt.radius = 4
            opt.fps = 20
            opt.data_root = './dataset/HumanML3D'
            opt.motion_dir = os.path.join(opt.data_root, 'new_joint_vecs')
            opt.text_dir = os.path.join(opt.data_root, 'texts')
            opt.mean_path = os.path.join(opt.data_root, 'Mean.npy')
            opt.std_path = os.path.join(opt.data_root, 'Std.npy')
            opt.split_dir = os.path.join(opt.data_root, 'train_val.txt')
            opt.meta_dir = './checkpoints/t2m/clip/meta'
            opt.eval_meta_dir = './dataset'
            opt.glove_dir = './dataset'
        elif dataset_name == 'kit':
            opt.joints_num = 21
            opt.dim_pose = 251
            opt.max_motion_length = 196
            opt.radius = 240 * 8
            opt.fps = 12.5
            opt.data_root = './dataset/KIT-ML'
            opt.motion_dir = os.path.join(opt.data_root, 'new_joint_vecs')
            opt.text_dir = os.path.join(opt.data_root, 'texts')
            opt.mean_path = os.path.join(opt.data_root, 'Mean.npy')
            opt.std_path = os.path.join(opt.data_root, 'Std.npy')
            opt.split_dir = os.path.join(opt.data_root, 'train_val_test.txt')
            opt.meta_dir = './checkpoints/kit/meta'
            opt.eval_meta_dir = './dataset'
            opt.glove_dir = './dataset'
    
    print(f"æ•°æ®é›†: {opt.dataset_name}")
    print(f"æ‰¹å¤„ç†å¤§å°: {opt.batch_size}")
    
    # åˆå§‹åŒ–CLIPæ¨¡å‹
    _init_clip_motion_model(model_path)
    model = GLOBAL_CACHE["clip_motion_align_model"]
    tokenizer = GLOBAL_CACHE["clip_tokenizer"]
    
    # æ ¹æ®å‚æ•°ç¡®å®šè¦æœç´¢çš„åˆ†å‰²
    if search_all_splits:
        splits_to_search = ['train', 'test']
        if dataset_name == 'kit':
            splits_to_search.append('val')
    else:
        splits_to_search = ['test']
    
    print(f"æœç´¢åˆ†å‰²: {splits_to_search}")
    
    # å­˜å‚¨æ‰€æœ‰åˆ†å‰²çš„åŒ¹é…æ•°æ®
    all_motion_embeddings_list = []
    all_text_embeddings_list = []
    all_filtered_captions = []
    total_found_count = 0
    total_processed_count = 0
    
    for split in splits_to_search:
        print(f"\n=== æœç´¢ {split} åˆ†å‰² ===")
        
        # è·å–å½“å‰åˆ†å‰²çš„æ•°æ®åŠ è½½å™¨
        try:
            split_loader = get_dataset_loader(
                opt,
                batch_size=opt.batch_size,
                split=split,
                mode='train'
            )
        except Exception as e:
            print(f"æ— æ³•åŠ è½½ {split} åˆ†å‰²: {e}")
            continue
        
        print(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸï¼Œæ€»æ‰¹æ¬¡æ•°: {len(split_loader)}")
        
        # å­˜å‚¨å½“å‰åˆ†å‰²çš„åŒ¹é…æ•°æ®
        split_motion_embeddings_list = []
        split_text_embeddings_list = []
        split_filtered_captions = []
        split_found_count = 0
        split_processed_count = 0
        
        print(f"å¼€å§‹æœç´¢ {split} åˆ†å‰²ä¸­åŒ…å« '{keyword}' çš„æ•°æ®...")
        
        for batch_idx, batch_data in enumerate(tqdm(split_loader, desc=f"æœç´¢{split}åˆ†å‰²")):
            torch.cuda.empty_cache()
            
            caption, motion, m_length = batch_data
            
            # ç­›é€‰åŒ…å«å…³é”®è¯çš„æ ·æœ¬ï¼ˆä½¿ç”¨æ›´çµæ´»çš„åŒ¹é…ï¼‰
            batch_filtered_indices = []
            batch_filtered_captions = []
            batch_filtered_motions = []
            batch_filtered_lengths = []
            
            for i, cap in enumerate(caption):
                cap_lower = cap.lower()
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•å…³é”®è¯å˜ä½“
                if any(variant in cap_lower for variant in keyword_variants):
                    batch_filtered_indices.append(i)
                    batch_filtered_captions.append(cap)
                    batch_filtered_motions.append(motion[i])
                    batch_filtered_lengths.append(m_length[i])
                    split_found_count += 1
            
            split_processed_count += len(caption)
            
            # å¦‚æœå½“å‰æ‰¹æ¬¡æœ‰åŒ¹é…çš„æ•°æ®ï¼Œè¿›è¡Œç¼–ç 
            if batch_filtered_indices:
                # å¤„ç†ç­›é€‰åçš„æ–‡æœ¬
                filtered_captions_lower = [cap.lower() for cap in batch_filtered_captions]
                text_enc = tokenizer(
                    filtered_captions_lower,
                    padding=True,
                    truncation=True,
                    max_length=opt.max_length,
                    return_tensors="pt"
                )
                input_ids = text_enc["input_ids"].to(opt.device)
                attention_mask = text_enc["attention_mask"].to(opt.device)
                
                # å¤„ç†ç­›é€‰åçš„motionæ•°æ®
                if isinstance(batch_filtered_motions[0], torch.Tensor):
                    filtered_motion_tensor = torch.stack(batch_filtered_motions, dim=0).float().to(opt.device)
                else:
                    filtered_motion_tensor = torch.stack([torch.tensor(m, dtype=torch.float32) for m in batch_filtered_motions], dim=0).to(opt.device)
                
                filtered_lengths_tensor = torch.tensor(batch_filtered_lengths, dtype=torch.long, device=opt.device)
                
                # å‰å‘ä¼ æ’­è·å–embeddings
                with torch.no_grad():
                    motion_emb, text_emb = model(filtered_motion_tensor, filtered_lengths_tensor, input_ids, attention_mask)
                
                # å­˜å‚¨ç»“æœ
                split_motion_embeddings_list.append(motion_emb.cpu().numpy())
                split_text_embeddings_list.append(text_emb.cpu().numpy())
                split_filtered_captions.extend(batch_filtered_captions)
                
                # æ¸…ç†å†…å­˜
                del motion_emb, text_emb
                torch.cuda.empty_cache()
            
            if (batch_idx + 1) % 50 == 0:
                print(f"å·²å¤„ç† {batch_idx + 1}/{len(split_loader)} æ‰¹æ¬¡ï¼Œæ‰¾åˆ° {split_found_count} ä¸ªåŒ¹é…æ ·æœ¬")
        
        print(f"{split} åˆ†å‰²æœç´¢å®Œæˆï¼åœ¨ {split_processed_count} ä¸ªæ ·æœ¬ä¸­æ‰¾åˆ° {split_found_count} ä¸ªåŒ…å« '{keyword}' çš„æ ·æœ¬")
        
        # åˆå¹¶å½“å‰åˆ†å‰²çš„ç»“æœ
        if split_motion_embeddings_list:
            all_motion_embeddings_list.extend(split_motion_embeddings_list)
            all_text_embeddings_list.extend(split_text_embeddings_list)
            all_filtered_captions.extend(split_filtered_captions)
        
        total_found_count += split_found_count
        total_processed_count += split_processed_count
    
    if total_found_count == 0:
        print(f"æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ '{keyword}' çš„æ•°æ®ï¼")
        return None, None, None
    
    print(f"\n=== æ€»ä½“æœç´¢ç»“æœ ===")
    print(f"åœ¨ {total_processed_count} ä¸ªæ ·æœ¬ä¸­æ‰¾åˆ° {total_found_count} ä¸ªåŒ…å« '{keyword}' çš„æ ·æœ¬")
    
    # åˆå¹¶æ‰€æœ‰ç¼–ç ç»“æœ
    all_motion_embeddings = np.concatenate(all_motion_embeddings_list, axis=0)
    all_text_embeddings = np.concatenate(all_text_embeddings_list, axis=0)
    
    print(f"Motion embeddingså½¢çŠ¶: {all_motion_embeddings.shape}")
    print(f"Text embeddingså½¢çŠ¶: {all_text_embeddings.shape}")
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    def cosine_similarity(a, b):
        """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    # è®¡ç®—æ¯å¯¹motion-textçš„ä½™å¼¦ç›¸ä¼¼åº¦
    similarities = []
    for i in range(len(all_motion_embeddings)):
        sim = cosine_similarity(all_motion_embeddings[i], all_text_embeddings[i])
        similarities.append(sim)
    
    similarities = np.array(similarities)
    mean_similarity = similarities.mean()
    std_similarity = similarities.std()
    
    print(f"\n=== ä½™å¼¦ç›¸ä¼¼åº¦ç»Ÿè®¡ ===")
    print(f"å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦: {mean_similarity:.6f}")
    print(f"æ ‡å‡†å·®: {std_similarity:.6f}")
    print(f"æœ€å°å€¼: {similarities.min():.6f}")
    print(f"æœ€å¤§å€¼: {similarities.max():.6f}")
    
    # ä¿å­˜ç»“æœ
    motion_output_path = os.path.join(output_dir, f"{keyword}_all_motion_embeddings.npy")
    text_output_path = os.path.join(output_dir, f"{keyword}_all_text_embeddings.npy")
    similarity_output_path = os.path.join(output_dir, f"{keyword}_all_similarities.npy")
    captions_output_path = os.path.join(output_dir, f"{keyword}_all_captions.txt")
    
    np.save(motion_output_path, all_motion_embeddings)
    np.save(text_output_path, all_text_embeddings)
    np.save(similarity_output_path, similarities)
    
    # ä¿å­˜å¯¹åº”çš„æ–‡æœ¬æè¿°
    with open(captions_output_path, 'w', encoding='utf-8') as f:
        for i, caption in enumerate(all_filtered_captions):
            f.write(f"{i}: {caption}\n")
    
    print(f"\n=== ä¿å­˜æ–‡ä»¶ ===")
    print(f"Motion embeddingsä¿å­˜åˆ°: {motion_output_path}")
    print(f"Text embeddingsä¿å­˜åˆ°: {text_output_path}")
    print(f"ç›¸ä¼¼åº¦ç»“æœä¿å­˜åˆ°: {similarity_output_path}")
    print(f"æ–‡æœ¬æè¿°ä¿å­˜åˆ°: {captions_output_path}")
    
    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
    print(f"\n=== ç¤ºä¾‹æ–‡æœ¬æè¿°ï¼ˆå‰10ä¸ªï¼‰===")
    for i, caption in enumerate(all_filtered_captions[:10]):
        print(f"{i+1}. {caption} (ç›¸ä¼¼åº¦: {similarities[i]:.6f})")
    
    if len(all_filtered_captions) > 10:
        print(f"... è¿˜æœ‰ {len(all_filtered_captions) - 10} ä¸ªæ ·æœ¬")
    
    return all_motion_embeddings, all_text_embeddings, similarities


def calculate_recall_at_k(motion_embeddings, text_embeddings, k_values=[1, 3, 5, 10]):
    """
    è®¡ç®—motion-to-textå’Œtext-to-motionçš„Recall@KæŒ‡æ ‡
    
    å‚æ•°ï¼š
        motion_embeddings: motionç¼–ç ï¼Œå½¢çŠ¶ä¸º (N, D)
        text_embeddings: textç¼–ç ï¼Œå½¢çŠ¶ä¸º (N, D)
        k_values: è¦è®¡ç®—çš„Kå€¼åˆ—è¡¨
    
    è¿”å›ï¼š
        dict: åŒ…å«å„ç§Recall@KæŒ‡æ ‡çš„å­—å…¸
    """
    
    # å½’ä¸€åŒ–embeddingsä»¥ä¾¿è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    motion_embeddings_norm = motion_embeddings / np.linalg.norm(motion_embeddings, axis=1, keepdims=True)
    text_embeddings_norm = text_embeddings / np.linalg.norm(text_embeddings, axis=1, keepdims=True)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    # motion_to_text: motion_embeddings_norm @ text_embeddings_norm.T
    # text_to_motion: text_embeddings_norm @ motion_embeddings_norm.T
    motion_to_text_sim = np.dot(motion_embeddings_norm, text_embeddings_norm.T)
    text_to_motion_sim = np.dot(text_embeddings_norm, motion_embeddings_norm.T)
    
    N = len(motion_embeddings)
    results = {}
    
    print(f"è®¡ç®— {N} ä¸ªæ ·æœ¬çš„Recall@KæŒ‡æ ‡...")
    
    for k in k_values:
        if k > N:
            print(f"K={k} å¤§äºæ ·æœ¬æ•°é‡ {N}ï¼Œè·³è¿‡")
            continue
            
        # Motion-to-Text Recall@K
        motion_to_text_recall = 0
        for i in range(N):
            # è·å–ç¬¬iä¸ªmotionä¸æ‰€æœ‰textçš„ç›¸ä¼¼åº¦
            similarities = motion_to_text_sim[i]
            # æ‰¾åˆ°top-kä¸ªæœ€ç›¸ä¼¼çš„textç´¢å¼•
            top_k_indices = np.argsort(similarities)[::-1][:k]
            # æ£€æŸ¥æ­£ç¡®çš„textï¼ˆç´¢å¼•iï¼‰æ˜¯å¦åœ¨top-kä¸­
            if i in top_k_indices:
                motion_to_text_recall += 1
        
        motion_to_text_recall /= N
        
        # Text-to-Motion Recall@K
        text_to_motion_recall = 0
        for i in range(N):
            # è·å–ç¬¬iä¸ªtextä¸æ‰€æœ‰motionçš„ç›¸ä¼¼åº¦
            similarities = text_to_motion_sim[i]
            # æ‰¾åˆ°top-kä¸ªæœ€ç›¸ä¼¼çš„motionç´¢å¼•
            top_k_indices = np.argsort(similarities)[::-1][:k]
            # æ£€æŸ¥æ­£ç¡®çš„motionï¼ˆç´¢å¼•iï¼‰æ˜¯å¦åœ¨top-kä¸­
            if i in top_k_indices:
                text_to_motion_recall += 1
        
        text_to_motion_recall /= N
        
        # å¹³å‡Recall@K
        avg_recall = (motion_to_text_recall + text_to_motion_recall) / 2
        
        results[f'Motion-to-Text R@{k}'] = motion_to_text_recall
        results[f'Text-to-Motion R@{k}'] = text_to_motion_recall
        results[f'Average R@{k}'] = avg_recall
        
        print(f"Recall@{k}:")
        print(f"  Motion-to-Text: {motion_to_text_recall:.4f} ({motion_to_text_recall*100:.2f}%)")
        print(f"  Text-to-Motion: {text_to_motion_recall:.4f} ({text_to_motion_recall*100:.2f}%)")
        print(f"  Average: {avg_recall:.4f} ({avg_recall*100:.2f}%)")
        print()
    
    return results


def evaluate_generation_quality(motion_embeddings, text_embeddings, captions):
    """
    è¯„ä¼°motionåˆ°textçš„ç”Ÿæˆè´¨é‡
    ç»™å®šmotionï¼Œç”Ÿæˆæœ€ç›¸ä¼¼çš„textï¼Œç„¶åè®¡ç®—ä¸çœŸå®textçš„ç›¸ä¼¼åº¦
    
    å‚æ•°ï¼š
        motion_embeddings: motionç¼–ç 
        text_embeddings: textç¼–ç   
        captions: å¯¹åº”çš„æ–‡æœ¬æè¿°
    """
    
    print("=== Motion-to-Text ç”Ÿæˆè´¨é‡è¯„ä¼° ===")
    
    # å½’ä¸€åŒ–embeddings
    motion_embeddings_norm = motion_embeddings / np.linalg.norm(motion_embeddings, axis=1, keepdims=True)
    text_embeddings_norm = text_embeddings / np.linalg.norm(text_embeddings, axis=1, keepdims=True)
    
    # è®¡ç®—motionåˆ°textçš„ç›¸ä¼¼åº¦çŸ©é˜µ
    motion_to_text_sim = np.dot(motion_embeddings_norm, text_embeddings_norm.T)
    
    N = len(motion_embeddings)
    generation_similarities = []
    top1_matches = 0
    top3_matches = 0
    top5_matches = 0
    
    print(f"è¯„ä¼° {N} ä¸ªmotionçš„ç”Ÿæˆè´¨é‡...")
    print("\n=== è¯¦ç»†ç”Ÿæˆç»“æœ ===")
    
    for i in range(N):
        # è·å–ç¬¬iä¸ªmotionä¸æ‰€æœ‰textçš„ç›¸ä¼¼åº¦
        similarities = motion_to_text_sim[i]
        
        # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„textï¼ˆç”Ÿæˆç»“æœï¼‰
        best_match_idx = np.argmax(similarities)
        best_similarity = similarities[best_match_idx]
        
        # ä¸çœŸå®textçš„ç›¸ä¼¼åº¦ï¼ˆå¯¹è§’çº¿å…ƒç´ ï¼‰
        true_similarity = similarities[i]
        
        generation_similarities.append(best_similarity)
        
        # è®¡ç®—æ’å
        sorted_indices = np.argsort(similarities)[::-1]
        true_rank = np.where(sorted_indices == i)[0][0] + 1
        
        if true_rank == 1:
            top1_matches += 1
        if true_rank <= 3:
            top3_matches += 1
        if true_rank <= 5:
            top5_matches += 1
        
        # è§£æcaption
        caption_parts = captions[i].split(': ', 1)
        true_caption = caption_parts[1] if len(caption_parts) > 1 else captions[i]
        
        best_caption_parts = captions[best_match_idx].split(': ', 1)
        generated_caption = best_caption_parts[1] if len(best_caption_parts) > 1 else captions[best_match_idx]
        
        print(f"\næ ·æœ¬ {i+1}:")
        print(f"  çœŸå®æè¿°: {true_caption}")
        print(f"  ç”Ÿæˆæè¿°: {generated_caption}")
        print(f"  ç”Ÿæˆç›¸ä¼¼åº¦: {best_similarity:.6f}")
        print(f"  çœŸå®ç›¸ä¼¼åº¦: {true_similarity:.6f}")
        print(f"  çœŸå®æ’å: {true_rank}")
        print(f"  åŒ¹é…çŠ¶æ€: {'âœ“' if best_match_idx == i else 'âœ—'}")
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    generation_similarities = np.array(generation_similarities)
    
    print(f"\n=== ç”Ÿæˆè´¨é‡ç»Ÿè®¡ ===")
    print(f"å¹³å‡ç”Ÿæˆç›¸ä¼¼åº¦: {generation_similarities.mean():.6f}")
    print(f"æ ‡å‡†å·®: {generation_similarities.std():.6f}")
    print(f"æœ€å°å€¼: {generation_similarities.min():.6f}")
    print(f"æœ€å¤§å€¼: {generation_similarities.max():.6f}")
    
    print(f"\n=== ç²¾ç¡®åŒ¹é…ç‡ ===")
    print(f"Top-1 ç²¾ç¡®åŒ¹é…: {top1_matches}/{N} = {top1_matches/N*100:.2f}%")
    print(f"Top-3 ç²¾ç¡®åŒ¹é…: {top3_matches}/{N} = {top3_matches/N*100:.2f}%")
    print(f"Top-5 ç²¾ç¡®åŒ¹é…: {top5_matches}/{N} = {top5_matches/N*100:.2f}%")
    
    # è®¡ç®—ç›¸ä¼¼åº¦åˆ†å¸ƒ
    high_quality = np.sum(generation_similarities >= 0.6)
    medium_quality = np.sum((generation_similarities >= 0.4) & (generation_similarities < 0.6))
    low_quality = np.sum(generation_similarities < 0.4)
    
    print(f"\n=== ç”Ÿæˆè´¨é‡åˆ†å¸ƒ ===")
    print(f"é«˜è´¨é‡ (â‰¥0.6): {high_quality}/{N} = {high_quality/N*100:.2f}%")
    print(f"ä¸­ç­‰è´¨é‡ (0.4-0.6): {medium_quality}/{N} = {medium_quality/N*100:.2f}%")
    print(f"ä½è´¨é‡ (<0.4): {low_quality}/{N} = {low_quality/N*100:.2f}%")
    
    return {
        'mean_similarity': generation_similarities.mean(),
        'std_similarity': generation_similarities.std(),
        'top1_accuracy': top1_matches/N,
        'top3_accuracy': top3_matches/N,
        'top5_accuracy': top5_matches/N,
        'high_quality_ratio': high_quality/N,
        'medium_quality_ratio': medium_quality/N,
        'low_quality_ratio': low_quality/N,
        'generation_similarities': generation_similarities
    }


def analyze_chicken_generation():
    """
    åˆ†æchickenæ•°æ®çš„ç”Ÿæˆè´¨é‡
    """
    print("=== åˆ†æChickenæ•°æ®çš„Motion-to-Textç”Ÿæˆè´¨é‡ ===")
    
    # åŠ è½½æ•°æ®
    try:
        motion_embeddings = np.load('./chicken_all_motion_embeddings.npy')
        text_embeddings = np.load('./chicken_all_text_embeddings.npy')
        similarities = np.load('./chicken_all_similarities.npy')
        
        with open('./chicken_all_captions.txt', 'r', encoding='utf-8') as f:
            captions = [line.strip() for line in f.readlines()]
            
        print(f"æˆåŠŸåŠ è½½æ•°æ®:")
        print(f"  Motion embeddings: {motion_embeddings.shape}")
        print(f"  Text embeddings: {text_embeddings.shape}")
        print(f"  æ ·æœ¬æ•°é‡: {len(captions)}")
        
    except FileNotFoundError as e:
        print(f"æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        print("è¯·å…ˆè¿è¡Œchickenæ•°æ®ç¼–ç ")
        return None
    
    # è¯„ä¼°ç”Ÿæˆè´¨é‡
    results = evaluate_generation_quality(motion_embeddings, text_embeddings, captions)
    
    print(f"\n=== æ€»ç»“è¯„ä¼° ===")
    print(f"å¯¹äº{len(captions)}ä¸ªåŒ…å«'chicken'çš„æ ·æœ¬ï¼š")
    print(f"- å¹³å‡ç”Ÿæˆç›¸ä¼¼åº¦: {results['mean_similarity']:.4f}")
    print(f"- Top-1 ç²¾ç¡®åŒ¹é…ç‡: {results['top1_accuracy']*100:.2f}%")
    print(f"- Top-3 ç²¾ç¡®åŒ¹é…ç‡: {results['top3_accuracy']*100:.2f}%")
    print(f"- é«˜è´¨é‡ç”Ÿæˆæ¯”ä¾‹: {results['high_quality_ratio']*100:.2f}%")
    
    # è´¨é‡è¯„ä¼°
    if results['mean_similarity'] >= 0.6:
        quality_level = "ä¼˜ç§€"
    elif results['mean_similarity'] >= 0.5:
        quality_level = "è‰¯å¥½"
    elif results['mean_similarity'] >= 0.4:
        quality_level = "ä¸­ç­‰"
    else:
        quality_level = "éœ€è¦æ”¹è¿›"
    
    print(f"- æ•´ä½“è´¨é‡è¯„çº§: {quality_level}")
    
    return results


def evaluate_full_dataset():
    """
    è¯„ä¼°æ•´ä¸ªæ•°æ®é›†çš„æ£€ç´¢æ€§èƒ½
    """
    print("=== è¯„ä¼°æ•´ä¸ªæ•°æ®é›†çš„æ£€ç´¢æ€§èƒ½ ===")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰å®Œæ•´æ•°æ®é›†çš„ç¼–ç 
    motion_file = "t2m_motion_embeddings.npy"
    
    if not os.path.exists(motion_file):
        print("å®Œæ•´æ•°æ®é›†ç¼–ç æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹ç”Ÿæˆ...")
        # é‡æ–°ç”Ÿæˆå®Œæ•´æ•°æ®é›†ç¼–ç ï¼Œä½†åŒæ—¶ä¿å­˜textç¼–ç 
        encode_full_dataset_with_text()
    
    # åŠ è½½å®Œæ•´æ•°æ®é›†ç¼–ç 
    try:
        motion_embeddings = np.load("t2m_full_motion_embeddings.npy")
        text_embeddings = np.load("t2m_full_text_embeddings.npy")
        
        print(f"æˆåŠŸåŠ è½½å®Œæ•´æ•°æ®é›†ç¼–ç :")
        print(f"  Motion embeddings: {motion_embeddings.shape}")
        print(f"  Text embeddings: {text_embeddings.shape}")
        
    except FileNotFoundError:
        print("å®Œæ•´æ•°æ®é›†ç¼–ç æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œæ­£åœ¨ç”Ÿæˆ...")
        encode_full_dataset_with_text()
        
        # é‡æ–°åŠ è½½
        motion_embeddings = np.load("t2m_full_motion_embeddings.npy")
        text_embeddings = np.load("t2m_full_text_embeddings.npy")
        
        print(f"æˆåŠŸç”Ÿæˆå¹¶åŠ è½½å®Œæ•´æ•°æ®é›†ç¼–ç :")
        print(f"  Motion embeddings: {motion_embeddings.shape}")
        print(f"  Text embeddings: {text_embeddings.shape}")
    
    # è®¡ç®—recall@kæŒ‡æ ‡
    print("\n=== è®¡ç®—å®Œæ•´æ•°æ®é›†çš„Recall@KæŒ‡æ ‡ ===")
    recall_results = calculate_recall_at_k(motion_embeddings, text_embeddings, k_values=[1, 3, 5, 10, 20, 50])
    
    # é‡ç‚¹å±•ç¤ºç»“æœ
    print(f"\n=== ğŸ¯ å®Œæ•´æ•°æ®é›†æ€§èƒ½æ€»ç»“ ===")
    print(f"æ•°æ®é›†è§„æ¨¡: {len(motion_embeddings)} ä¸ªæ ·æœ¬")
    print(f"")
    print(f"ğŸ“Š ä¸»è¦æŒ‡æ ‡:")
    print(f"  Motion-to-Text R@1:  {recall_results['Motion-to-Text R@1']*100:.2f}%")
    print(f"  Motion-to-Text R@3:  {recall_results['Motion-to-Text R@3']*100:.2f}%")
    print(f"  Motion-to-Text R@5:  {recall_results['Motion-to-Text R@5']*100:.2f}%")
    print(f"  Motion-to-Text R@10: {recall_results['Motion-to-Text R@10']*100:.2f}%")
    print(f"")
    print(f"  Text-to-Motion R@1:  {recall_results['Text-to-Motion R@1']*100:.2f}%")
    print(f"  Text-to-Motion R@3:  {recall_results['Text-to-Motion R@3']*100:.2f}%")
    print(f"  Text-to-Motion R@5:  {recall_results['Text-to-Motion R@5']*100:.2f}%")
    print(f"  Text-to-Motion R@10: {recall_results['Text-to-Motion R@10']*100:.2f}%")
    print(f"")
    print(f"ğŸ¯ é‡ç‚¹ï¼šå¹³å‡ R@3 = {recall_results['Average R@3']*100:.2f}%")
    
    # æ€§èƒ½è¯„ä¼°
    avg_r3 = recall_results['Average R@3']
    if avg_r3 >= 0.4:
        performance_level = "ä¼˜ç§€"
        emoji = "ğŸ†"
    elif avg_r3 >= 0.3:
        performance_level = "è‰¯å¥½"
        emoji = "âœ…"
    elif avg_r3 >= 0.2:
        performance_level = "ä¸­ç­‰"
        emoji = "ğŸ“ˆ"
    else:
        performance_level = "éœ€è¦æ”¹è¿›"
        emoji = "âš ï¸"
    
    print(f"\n{emoji} ç¼–ç å™¨æ€§èƒ½è¯„çº§: {performance_level}")
    
    return recall_results


def encode_full_dataset_with_text(opt_path=None, dataset_name='t2m', 
                                 model_path="/data/wenshuo/project/jhz/StableMoFusion/data/clip_motion_align_epoch_21.pt", 
                                 batch_size=32):
    """
    ç¼–ç å®Œæ•´æ•°æ®é›†çš„motionå’Œtextå¹¶ä¿å­˜
    """
    
    # è®¾ç½®device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    set_global_device(device)
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆå§‹åŒ–å‚æ•°
    opt = Namespace()
    if opt_path and os.path.exists(opt_path):
        get_opt(opt, opt_path)
    else:
        # ä½¿ç”¨é»˜è®¤é…ç½®
        opt.dataset_name = dataset_name
        opt.batch_size = batch_size
        opt.device = device
        opt.max_length = 77
        opt.feat_bias = 5
        opt.max_text_len = 20
        opt.unit_length = 4
        
        if dataset_name == 't2m':
            opt.joints_num = 22
            opt.dim_pose = 263
            opt.max_motion_length = 196
            opt.radius = 4
            opt.fps = 20
            opt.data_root = './dataset/HumanML3D'
            opt.motion_dir = os.path.join(opt.data_root, 'new_joint_vecs')
            opt.text_dir = os.path.join(opt.data_root, 'texts')
            opt.mean_path = os.path.join(opt.data_root, 'Mean.npy')
            opt.std_path = os.path.join(opt.data_root, 'Std.npy')
            opt.split_dir = os.path.join(opt.data_root, 'train_val.txt')
            opt.meta_dir = './checkpoints/t2m/clip/meta'
            opt.eval_meta_dir = './dataset'
            opt.glove_dir = './dataset'
    
    print(f"æ•°æ®é›†: {opt.dataset_name}")
    print(f"æ‰¹å¤„ç†å¤§å°: {opt.batch_size}")
    
    # åˆå§‹åŒ–CLIPæ¨¡å‹
    _init_clip_motion_model(model_path)
    model = GLOBAL_CACHE["clip_motion_align_model"]
    tokenizer = GLOBAL_CACHE["clip_tokenizer"]
    
    # å¤„ç†æµ‹è¯•é›†
    test_loader = get_dataset_loader(
        opt,
        batch_size=opt.batch_size,
        split='test',
        mode='train'
    )
    
    print(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸï¼Œæ€»æ‰¹æ¬¡æ•°: {len(test_loader)}")
    
    # å­˜å‚¨æ‰€æœ‰embeddings
    all_motion_embeddings = []
    all_text_embeddings = []
    
    print("å¼€å§‹ç¼–ç å®Œæ•´æ•°æ®é›†...")
    for batch_idx, batch_data in enumerate(tqdm(test_loader, desc="ç¼–ç è¿›åº¦")):
        torch.cuda.empty_cache()
        
        caption, motion, m_length = batch_data
        
        # å¤„ç†æ–‡æœ¬
        caption = [c.lower() for c in caption]
        text_enc = tokenizer(
            caption,
            padding=True,
            truncation=True,
            max_length=opt.max_length,
            return_tensors="pt"
        )
        input_ids = text_enc["input_ids"].to(opt.device)
        attention_mask = text_enc["attention_mask"].to(opt.device)
        
        # å¤„ç†motionæ•°æ®
        if isinstance(motion, list):
            motion = torch.stack([torch.tensor(m, dtype=torch.float32) for m in motion], dim=0)
        else:
            motion = motion.float()
        motion = motion.to(opt.device)
        m_length = m_length.to(opt.device)
        
        # å‰å‘ä¼ æ’­è·å–embeddings
        with torch.no_grad():
            motion_emb, text_emb = model(motion, m_length, input_ids, attention_mask)
        
        # ä¿å­˜embeddings
        all_motion_embeddings.append(motion_emb.cpu().numpy())
        all_text_embeddings.append(text_emb.cpu().numpy())
        
        # æ¸…ç†å†…å­˜
        del motion_emb, text_emb
        torch.cuda.empty_cache()
        
        if (batch_idx + 1) % 50 == 0:
            print(f"å·²å¤„ç† {batch_idx + 1}/{len(test_loader)} æ‰¹æ¬¡")
    
    # åˆå¹¶æ‰€æœ‰embeddings
    full_motion_embeddings = np.concatenate(all_motion_embeddings, axis=0)
    full_text_embeddings = np.concatenate(all_text_embeddings, axis=0)
    
    print(f"ç¼–ç å®Œæˆï¼")
    print(f"Motion embeddingså½¢çŠ¶: {full_motion_embeddings.shape}")
    print(f"Text embeddingså½¢çŠ¶: {full_text_embeddings.shape}")
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    np.save("t2m_full_motion_embeddings.npy", full_motion_embeddings)
    np.save("t2m_full_text_embeddings.npy", full_text_embeddings)
    
    print(f"å·²ä¿å­˜åˆ°:")
    print(f"  t2m_full_motion_embeddings.npy")
    print(f"  t2m_full_text_embeddings.npy")
    
    # æ¸…ç†å†…å­˜
    del all_motion_embeddings, all_text_embeddings
    torch.cuda.empty_cache()
    
    return full_motion_embeddings, full_text_embeddings


def evaluate_random_subset(n_samples=32, n_trials=5):
    """
    ä»å®Œæ•´æ•°æ®é›†ä¸­éšæœºé€‰æ‹©n_samplesä¸ªæ ·æœ¬ï¼Œè®¡ç®—R@3ï¼Œé‡å¤n_trialsæ¬¡æ±‚å¹³å‡
    
    å‚æ•°ï¼š
        n_samples: é€‰æ‹©çš„æ ·æœ¬æ•°é‡
        n_trials: é‡å¤è¯•éªŒæ¬¡æ•°
    """
    print(f"=== éšæœºé€‰æ‹©{n_samples}ä¸ªæ ·æœ¬è¯„ä¼°R@3æ€§èƒ½ ===")
    
    # åŠ è½½å®Œæ•´æ•°æ®é›†ç¼–ç 
    try:
        motion_embeddings = np.load("t2m_full_motion_embeddings.npy")
        text_embeddings = np.load("t2m_full_text_embeddings.npy")
        
        print(f"æˆåŠŸåŠ è½½å®Œæ•´æ•°æ®é›†ç¼–ç :")
        print(f"  Motion embeddings: {motion_embeddings.shape}")
        print(f"  Text embeddings: {text_embeddings.shape}")
        
    except FileNotFoundError:
        print("å®Œæ•´æ•°æ®é›†ç¼–ç æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œå®Œæ•´æ•°æ®é›†ç¼–ç ")
        return None
    
    total_samples = len(motion_embeddings)
    print(f"ä»{total_samples}ä¸ªæ ·æœ¬ä¸­éšæœºé€‰æ‹©{n_samples}ä¸ªï¼Œé‡å¤{n_trials}æ¬¡")
    
    # å­˜å‚¨æ¯æ¬¡è¯•éªŒçš„ç»“æœ
    trial_results = []
    
    for trial in range(n_trials):
        print(f"\n--- ç¬¬{trial+1}æ¬¡è¯•éªŒ ---")
        
        # éšæœºé€‰æ‹©æ ·æœ¬ç´¢å¼•
        np.random.seed(trial)  # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤
        selected_indices = np.random.choice(total_samples, n_samples, replace=False)
        
        # é€‰æ‹©å¯¹åº”çš„embeddings
        selected_motion_embs = motion_embeddings[selected_indices]
        selected_text_embs = text_embeddings[selected_indices]
        
        print(f"é€‰ä¸­æ ·æœ¬ç´¢å¼•: {selected_indices[:10]}..." if n_samples > 10 else f"é€‰ä¸­æ ·æœ¬ç´¢å¼•: {selected_indices}")
        
        # è®¡ç®—è¿™ä¸ªå­é›†çš„R@3
        subset_results = calculate_recall_at_k(
            selected_motion_embs, 
            selected_text_embs, 
            k_values=[1, 3, 5]
        )
        
        trial_results.append(subset_results)
        
        print(f"æœ¬æ¬¡è¯•éªŒç»“æœ:")
        print(f"  Motion-to-Text R@3: {subset_results['Motion-to-Text R@3']*100:.2f}%")
        print(f"  Text-to-Motion R@3: {subset_results['Text-to-Motion R@3']*100:.2f}%")
        print(f"  å¹³å‡ R@3: {subset_results['Average R@3']*100:.2f}%")
    
    # è®¡ç®—æ‰€æœ‰è¯•éªŒçš„å¹³å‡å€¼
    print(f"\n=== {n_trials}æ¬¡è¯•éªŒçš„ç»Ÿè®¡ç»“æœ ===")
    
    # æ”¶é›†æ‰€æœ‰è¯•éªŒçš„æŒ‡æ ‡
    motion_to_text_r1 = [r['Motion-to-Text R@1'] for r in trial_results]
    motion_to_text_r3 = [r['Motion-to-Text R@3'] for r in trial_results]
    motion_to_text_r5 = [r['Motion-to-Text R@5'] for r in trial_results]
    
    text_to_motion_r1 = [r['Text-to-Motion R@1'] for r in trial_results]
    text_to_motion_r3 = [r['Text-to-Motion R@3'] for r in trial_results]
    text_to_motion_r5 = [r['Text-to-Motion R@5'] for r in trial_results]
    
    avg_r1 = [r['Average R@1'] for r in trial_results]
    avg_r3 = [r['Average R@3'] for r in trial_results]
    avg_r5 = [r['Average R@5'] for r in trial_results]
    
    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    print(f"ğŸ“Š Motion-to-Text:")
    print(f"  R@1: {np.mean(motion_to_text_r1)*100:.2f}% Â± {np.std(motion_to_text_r1)*100:.2f}%")
    print(f"  R@3: {np.mean(motion_to_text_r3)*100:.2f}% Â± {np.std(motion_to_text_r3)*100:.2f}%")
    print(f"  R@5: {np.mean(motion_to_text_r5)*100:.2f}% Â± {np.std(motion_to_text_r5)*100:.2f}%")
    
    print(f"ğŸ“Š Text-to-Motion:")
    print(f"  R@1: {np.mean(text_to_motion_r1)*100:.2f}% Â± {np.std(text_to_motion_r1)*100:.2f}%")
    print(f"  R@3: {np.mean(text_to_motion_r3)*100:.2f}% Â± {np.std(text_to_motion_r3)*100:.2f}%")
    print(f"  R@5: {np.mean(text_to_motion_r5)*100:.2f}% Â± {np.std(text_to_motion_r5)*100:.2f}%")
    
    print(f"ğŸ¯ å¹³å‡æŒ‡æ ‡:")
    print(f"  R@1: {np.mean(avg_r1)*100:.2f}% Â± {np.std(avg_r1)*100:.2f}%")
    print(f"  R@3: {np.mean(avg_r3)*100:.2f}% Â± {np.std(avg_r3)*100:.2f}%")
    print(f"  R@5: {np.mean(avg_r5)*100:.2f}% Â± {np.std(avg_r5)*100:.2f}%")
    
    # é‡ç‚¹æ€»ç»“
    final_r3_mean = np.mean(avg_r3)
    final_r3_std = np.std(avg_r3)
    
    print(f"\nğŸ† æœ€ç»ˆç»“æœ:")
    print(f"åœ¨{n_samples}ä¸ªæ ·æœ¬çš„å­é›†ä¸Šï¼Œ")
    print(f"å¹³å‡ R@3 = {final_r3_mean*100:.2f}% Â± {final_r3_std*100:.2f}%")
    
    # æ€§èƒ½è¯„çº§
    if final_r3_mean >= 0.4:
        performance_level = "ä¼˜ç§€"
        emoji = "ğŸ†"
    elif final_r3_mean >= 0.3:
        performance_level = "è‰¯å¥½"
        emoji = "âœ…"
    elif final_r3_mean >= 0.2:
        performance_level = "ä¸­ç­‰"
        emoji = "ğŸ“ˆ"
    else:
        performance_level = "éœ€è¦æ”¹è¿›"
        emoji = "âš ï¸"
    
    print(f"{emoji} ç¼–ç å™¨æ€§èƒ½è¯„çº§: {performance_level}")
    
    return {
        'mean_r3': final_r3_mean,
        'std_r3': final_r3_std,
        'all_results': trial_results,
        'n_samples': n_samples,
        'n_trials': n_trials
    }


def encode_keyword_motions_with_new_model(keyword, opt_path=None, dataset_name='t2m', 
                          new_model_path="./checkpoints/moclip_training/best_model.pt", 
                          old_model_path="/data/wenshuo/project/jhz/StableMoFusion/data/clip_motion_align_epoch_21.pt",
                          output_dir="./", batch_size=32, search_all_splits=True):
    """
    ä½¿ç”¨æ–°è®­ç»ƒçš„æ¨¡å‹ç¼–ç ç‰¹å®šå…³é”®è¯çš„motionå’Œtextï¼Œå¹¶ä¸æ—§æ¨¡å‹å¯¹æ¯”
    
    å‚æ•°ï¼š
        keyword: è¦æœç´¢çš„å…³é”®è¯
        new_model_path: æ–°è®­ç»ƒçš„æ¨¡å‹è·¯å¾„
        old_model_path: åŸå§‹æ¨¡å‹è·¯å¾„
        å…¶ä»–å‚æ•°åŒencode_keyword_motions
    """
    
    print(f"=== ğŸ†š æ–°æ—§æ¨¡å‹å¯¹æ¯”ï¼šå…³é”®è¯ '{keyword}' ===")
    
    # é¦–å…ˆç”¨æ–°æ¨¡å‹ç¼–ç 
    print(f"\nğŸ”¥ ä½¿ç”¨æ–°è®­ç»ƒçš„æ¨¡å‹: {new_model_path}")
    new_motion_embs, new_text_embs, new_similarities = encode_keyword_motions(
        keyword=keyword,
        opt_path=opt_path,
        dataset_name=dataset_name,
        model_path=new_model_path,
        output_dir=output_dir,
        batch_size=batch_size,
        search_all_splits=search_all_splits
    )
    
    if new_motion_embs is None:
        print("æ–°æ¨¡å‹ç¼–ç å¤±è´¥")
        return None
    
    # è¯»å–æ–°æ¨¡å‹çš„captionsç”¨äºåç»­å¯¹æ¯”
    with open(f"{output_dir}/{keyword}_all_captions.txt", 'r', encoding='utf-8') as f:
        new_captions = [line.strip() for line in f.readlines()]
    
    # æ¸…ç†å…¨å±€ç¼“å­˜ï¼Œå‡†å¤‡åŠ è½½æ—§æ¨¡å‹
    global GLOBAL_CACHE
    GLOBAL_CACHE = {
        "clip_model": None,
        "clip_tokenizer": None,
        "motion_encoder": None,
        "clip_motion_align_model": None,
        "device": None
    }
    
    print(f"\nğŸ“Š ä½¿ç”¨åŸå§‹æ¨¡å‹å¯¹æ¯”: {old_model_path}")
    old_motion_embs, old_text_embs, old_similarities = encode_keyword_motions(
        keyword=keyword,
        opt_path=opt_path,
        dataset_name=dataset_name,
        model_path=old_model_path,
        output_dir=output_dir + "/old_model/",
        batch_size=batch_size,
        search_all_splits=search_all_splits
    )
    
    if old_motion_embs is None:
        print("åŸå§‹æ¨¡å‹ç¼–ç å¤±è´¥")
        return new_motion_embs, new_text_embs, new_similarities
    
    # è¯»å–åŸå§‹æ¨¡å‹çš„captions
    with open(f"{output_dir}/old_model/{keyword}_all_captions.txt", 'r', encoding='utf-8') as f:
        old_captions = [line.strip() for line in f.readlines()]
    
    # å¯¹æ¯”åˆ†æ
    print(f"\n=== ğŸ“ˆ æ€§èƒ½å¯¹æ¯”åˆ†æ ===")
    print(f"å…³é”®è¯: '{keyword}'")
    print(f"æ–°æ¨¡å‹æ ·æœ¬æ•°é‡: {len(new_similarities)}")
    print(f"åŸå§‹æ¨¡å‹æ ·æœ¬æ•°é‡: {len(old_similarities)}")
    print(f"")
    
    # ä½™å¼¦ç›¸ä¼¼åº¦å¯¹æ¯”
    print(f"ğŸ“Š å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦å¯¹æ¯”:")
    print(f"  æ–°æ¨¡å‹: {new_similarities.mean():.6f} Â± {new_similarities.std():.6f}")
    print(f"  åŸå§‹æ¨¡å‹: {old_similarities.mean():.6f} Â± {old_similarities.std():.6f}")
    improvement = new_similarities.mean() - old_similarities.mean()
    print(f"  æ”¹è¿›: {improvement:+.6f} ({improvement/old_similarities.mean()*100:+.2f}%)")
    
    # è´¨é‡åˆ†å¸ƒå¯¹æ¯”
    def analyze_quality_distribution(similarities, model_name):
        high_quality = np.sum(similarities >= 0.6)
        medium_quality = np.sum((similarities >= 0.4) & (similarities < 0.6))
        low_quality = np.sum(similarities < 0.4)
        total = len(similarities)
        
        print(f"  {model_name}:")
        print(f"    é«˜è´¨é‡ (â‰¥0.6): {high_quality}/{total} = {high_quality/total*100:.1f}%")
        print(f"    ä¸­ç­‰è´¨é‡ (0.4-0.6): {medium_quality}/{total} = {medium_quality/total*100:.1f}%")
        print(f"    ä½è´¨é‡ (<0.4): {low_quality}/{total} = {low_quality/total*100:.1f}%")
        return high_quality/total, medium_quality/total, low_quality/total
    
    print(f"\nğŸ“Š è´¨é‡åˆ†å¸ƒå¯¹æ¯”:")
    new_high, new_mid, new_low = analyze_quality_distribution(new_similarities, "æ–°æ¨¡å‹")
    old_high, old_mid, old_low = analyze_quality_distribution(old_similarities, "åŸå§‹æ¨¡å‹")
    
    # é«˜è´¨é‡æ ·æœ¬çš„æ”¹è¿›
    high_quality_improvement = new_high - old_high
    print(f"\nğŸ¯ é«˜è´¨é‡æ ·æœ¬æ¯”ä¾‹æ”¹è¿›: {high_quality_improvement:+.1%}")
    
    # å¦‚æœæ ·æœ¬æ•°é‡ç›¸åŒï¼Œæ˜¾ç¤ºé€æ ·æœ¬å¯¹æ¯”ï¼Œå¦åˆ™è·³è¿‡
    if len(new_similarities) == len(old_similarities):
        print(f"\nğŸ† æ ·æœ¬æ•°é‡ä¸€è‡´ï¼Œè¿›è¡Œé€æ ·æœ¬å¯¹æ¯”...")
        sample_improvements = new_similarities - old_similarities
        best_improvements = np.argsort(sample_improvements)[::-1][:5]
        
        print(f"\nğŸ† æ”¹è¿›æœ€å¤§çš„5ä¸ªæ ·æœ¬:")
        for i, idx in enumerate(best_improvements, 1):
            caption_parts = new_captions[idx].split(': ', 1)
            caption_text = caption_parts[1] if len(caption_parts) > 1 else new_captions[idx]
            improvement_val = sample_improvements[idx]
            print(f"  {i}. {caption_text}")
            print(f"     æ”¹è¿›: {improvement_val:+.6f} (æ–°:{new_similarities[idx]:.6f} vs æ—§:{old_similarities[idx]:.6f})")
    else:
        print(f"\nâš ï¸ ä¸¤ä¸ªæ¨¡å‹æ‰¾åˆ°çš„æ ·æœ¬æ•°é‡ä¸åŒ (æ–°:{len(new_similarities)} vs æ—§:{len(old_similarities)})ï¼Œè·³è¿‡é€æ ·æœ¬å¯¹æ¯”")
        print(f"å¯èƒ½åŸå› ï¼šä¸åŒçš„æœç´¢æ‰¹æ¬¡æˆ–æ•°æ®åŠ è½½é¡ºåºå¯¼è‡´çš„å¾®å°å·®å¼‚")
    
    # ä¿å­˜å¯¹æ¯”ç»“æœ
    comparison_results = {
        'keyword': keyword,
        'new_n_samples': len(new_similarities),
        'old_n_samples': len(old_similarities),
        'new_model_path': new_model_path,
        'old_model_path': old_model_path,
        'new_mean_similarity': float(new_similarities.mean()),
        'old_mean_similarity': float(old_similarities.mean()),
        'improvement': float(improvement),
        'improvement_percent': float(improvement/old_similarities.mean()*100),
        'new_high_quality_ratio': float(new_high),
        'old_high_quality_ratio': float(old_high),
        'high_quality_improvement': float(high_quality_improvement)
    }
    
    import json
    with open(f"{output_dir}/{keyword}_model_comparison.json", 'w') as f:
        json.dump(comparison_results, f, indent=2)
    
    print(f"\nğŸ’¾ å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {output_dir}/{keyword}_model_comparison.json")
    
    # æ€»ç»“
    if improvement > 0:
        if improvement > 0.05:
            performance_level = "æ˜¾è‘—æå‡"
            emoji = "ğŸš€"
        elif improvement > 0.02:
            performance_level = "æ˜æ˜¾æå‡"
            emoji = "ğŸ“ˆ"
        else:
            performance_level = "å°å¹…æå‡"
            emoji = "â¬†ï¸"
    else:
        performance_level = "éœ€è¦è°ƒä¼˜"
        emoji = "âš ï¸"
    
    print(f"\n{emoji} æ€»ä½“è¯„ä»·: {performance_level}")
    print(f"æ–°è®­ç»ƒçš„æ¨¡å‹åœ¨'{keyword}'æ•°æ®ä¸Šçš„è¡¨ç°ç›¸æ¯”åŸå§‹æ¨¡å‹æœ‰{improvement/old_similarities.mean()*100:+.1f}%çš„æ”¹è¿›")
    
    return new_motion_embs, new_text_embs, new_similarities, comparison_results


def fair_model_comparison(keyword, opt_path=None, dataset_name='t2m', 
                         new_model_path="./checkpoints/moclip_training/best_model.pt", 
                         old_model_path="/data/wenshuo/project/jhz/StableMoFusion/data/clip_motion_align_epoch_21.pt",
                         output_dir="./", batch_size=32, search_all_splits=True):
    """
    å…¬å¹³å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹ï¼šç¡®ä¿ä¸¤ä¸ªæ¨¡å‹å¤„ç†å®Œå…¨ç›¸åŒçš„æ ·æœ¬é›†
    
    å‚æ•°ï¼š
        keyword: è¦æœç´¢çš„å…³é”®è¯
        å…¶ä»–å‚æ•°åŒencode_keyword_motions
    """
    
    print(f"=== ğŸ¯ å…¬å¹³å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹ï¼šå…³é”®è¯ '{keyword}' ===")
    
    # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰åŒ¹é…çš„æ ·æœ¬ï¼ˆä¸ç¼–ç ï¼‰
    print(f"\nğŸ“‹ ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰åŒ…å«'{keyword}'çš„æ ·æœ¬...")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    set_global_device(device)
    
    # å…³é”®è¯å˜ä½“
    keyword_variants = [
        keyword.lower(),
        keyword.lower() + 's',
        keyword.lower() + '-like',
        keyword.lower() + ' like',
        'like a ' + keyword.lower(),
        'like ' + keyword.lower(),
    ]
    
    # åˆå§‹åŒ–å‚æ•°
    opt = Namespace()
    opt.dataset_name = dataset_name
    opt.batch_size = batch_size
    opt.device = device
    opt.max_length = 77
    opt.feat_bias = 5
    opt.max_text_len = 20
    opt.unit_length = 4
    
    if dataset_name == 't2m':
        opt.joints_num = 22
        opt.dim_pose = 263
        opt.max_motion_length = 196
        opt.radius = 4
        opt.fps = 20
        opt.data_root = './dataset/HumanML3D'
        opt.motion_dir = os.path.join(opt.data_root, 'new_joint_vecs')
        opt.text_dir = os.path.join(opt.data_root, 'texts')
        opt.mean_path = os.path.join(opt.data_root, 'Mean.npy')
        opt.std_path = os.path.join(opt.data_root, 'Std.npy')
        opt.split_dir = os.path.join(opt.data_root, 'train_val.txt')
        opt.meta_dir = './checkpoints/t2m/clip/meta'
        opt.eval_meta_dir = './dataset'
        opt.glove_dir = './dataset'
    
    # æ”¶é›†æ‰€æœ‰åŒ¹é…çš„æ ·æœ¬
    all_matched_samples = []
    
    splits_to_search = ['train', 'test'] if search_all_splits else ['test']
    
    for split in splits_to_search:
        print(f"\n--- æ”¶é›† {split} åˆ†å‰²çš„æ ·æœ¬ ---")
        
        # è®¾ç½®å›ºå®šéšæœºç§å­ç¡®ä¿ç¡®å®šæ€§
        torch.manual_seed(42)
        np.random.seed(42)
        
        split_loader = get_dataset_loader(
            opt,
            batch_size=opt.batch_size,
            split=split,
            mode='train'
        )
        
        for batch_idx, batch_data in enumerate(tqdm(split_loader, desc=f"æ”¶é›†{split}åˆ†å‰²")):
            captions, motions, lengths = batch_data
            
            # ç­›é€‰åŒ…å«å…³é”®è¯çš„æ ·æœ¬
            for i, caption in enumerate(captions):
                cap_lower = caption.lower()
                if any(variant in cap_lower for variant in keyword_variants):
                    # ä¿å­˜æ ·æœ¬ä¿¡æ¯
                    sample_info = {
                        'caption': caption,
                        'motion': motions[i] if isinstance(motions, list) else motions[i].clone().float(),
                        'length': lengths[i].item() if hasattr(lengths[i], 'item') else lengths[i],
                        'split': split,
                        'batch_idx': batch_idx,
                        'sample_idx': i
                    }
                    all_matched_samples.append(sample_info)
    
    print(f"\nâœ… æ€»å…±æ”¶é›†åˆ° {len(all_matched_samples)} ä¸ªåŒ¹é…æ ·æœ¬")
    
    # å»é‡ï¼ˆåŸºäºcaptionï¼‰
    unique_samples = []
    seen_captions = set()
    for sample in all_matched_samples:
        if sample['caption'] not in seen_captions:
            unique_samples.append(sample)
            seen_captions.add(sample['caption'])
        
    print(f"âœ… å»é‡åå‰©ä½™ {len(unique_samples)} ä¸ªç‹¬ç‰¹æ ·æœ¬")
    
    # ç¬¬äºŒæ­¥ï¼šç”¨æ–°æ¨¡å‹ç¼–ç 
    print(f"\nğŸ”¥ ç¬¬äºŒæ­¥ï¼šä½¿ç”¨æ–°æ¨¡å‹ç¼–ç æ ·æœ¬...")
    new_motion_embeddings, new_text_embeddings = encode_samples_with_model(
        unique_samples, new_model_path, opt
    )
    
    # ç¬¬ä¸‰æ­¥ï¼šç”¨åŸå§‹æ¨¡å‹ç¼–ç 
    print(f"\nğŸ“Š ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨åŸå§‹æ¨¡å‹ç¼–ç ç›¸åŒæ ·æœ¬...")
    
    # æ¸…ç†å…¨å±€ç¼“å­˜
    global GLOBAL_CACHE
    GLOBAL_CACHE = {
        "clip_model": None,
        "clip_tokenizer": None, 
        "motion_encoder": None,
        "clip_motion_align_model": None,
        "device": None
    }
    
    old_motion_embeddings, old_text_embeddings = encode_samples_with_model(
        unique_samples, old_model_path, opt
    )
    
    # ç¬¬å››æ­¥ï¼šè®¡ç®—ç›¸ä¼¼åº¦å¹¶å¯¹æ¯”
    print(f"\nğŸ“ˆ ç¬¬å››æ­¥ï¼šå¯¹æ¯”åˆ†æ...")
    
    new_similarities = np.array([
        np.dot(new_motion_embeddings[i], new_text_embeddings[i]) / 
        (np.linalg.norm(new_motion_embeddings[i]) * np.linalg.norm(new_text_embeddings[i]))
        for i in range(len(unique_samples))
    ])
    
    old_similarities = np.array([
        np.dot(old_motion_embeddings[i], old_text_embeddings[i]) / 
        (np.linalg.norm(old_motion_embeddings[i]) * np.linalg.norm(old_text_embeddings[i]))
        for i in range(len(unique_samples))
    ])
    
    # è¯¦ç»†å¯¹æ¯”åˆ†æ
    print(f"\n=== ğŸ“ˆ å…¬å¹³å¯¹æ¯”ç»“æœ ===")
    print(f"å…³é”®è¯: '{keyword}'")
    print(f"æ ·æœ¬æ•°é‡: {len(unique_samples)} (å®Œå…¨ç›¸åŒ)")
    print(f"")
    
    # ä½™å¼¦ç›¸ä¼¼åº¦å¯¹æ¯”
    print(f"ğŸ“Š å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦å¯¹æ¯”:")
    print(f"  æ–°æ¨¡å‹: {new_similarities.mean():.6f} Â± {new_similarities.std():.6f}")
    print(f"  åŸå§‹æ¨¡å‹: {old_similarities.mean():.6f} Â± {old_similarities.std():.6f}")
    improvement = new_similarities.mean() - old_similarities.mean()
    print(f"  æ”¹è¿›: {improvement:+.6f} ({improvement/old_similarities.mean()*100:+.2f}%)")
    
    # è´¨é‡åˆ†å¸ƒå¯¹æ¯”
    def analyze_quality_distribution(similarities, model_name):
        high_quality = np.sum(similarities >= 0.6)
        medium_quality = np.sum((similarities >= 0.4) & (similarities < 0.6))
        low_quality = np.sum(similarities < 0.4)
        total = len(similarities)
        
        print(f"  {model_name}:")
        print(f"    é«˜è´¨é‡ (â‰¥0.6): {high_quality}/{total} = {high_quality/total*100:.1f}%")
        print(f"    ä¸­ç­‰è´¨é‡ (0.4-0.6): {medium_quality}/{total} = {medium_quality/total*100:.1f}%")
        print(f"    ä½è´¨é‡ (<0.4): {low_quality}/{total} = {low_quality/total*100:.1f}%")
        return high_quality/total, medium_quality/total, low_quality/total
    
    print(f"\nğŸ“Š è´¨é‡åˆ†å¸ƒå¯¹æ¯”:")
    new_high, new_mid, new_low = analyze_quality_distribution(new_similarities, "æ–°æ¨¡å‹")
    old_high, old_mid, old_low = analyze_quality_distribution(old_similarities, "åŸå§‹æ¨¡å‹")
    
    # é«˜è´¨é‡æ ·æœ¬çš„æ”¹è¿›
    high_quality_improvement = new_high - old_high
    print(f"\nğŸ¯ é«˜è´¨é‡æ ·æœ¬æ¯”ä¾‹æ”¹è¿›: {high_quality_improvement:+.1%}")
    
    # é€æ ·æœ¬å¯¹æ¯”
    sample_improvements = new_similarities - old_similarities
    best_improvements = np.argsort(sample_improvements)[::-1][:5]
    worst_improvements = np.argsort(sample_improvements)[:5]
    
    print(f"\nğŸ† æ”¹è¿›æœ€å¤§çš„5ä¸ªæ ·æœ¬:")
    for i, idx in enumerate(best_improvements, 1):
        caption = unique_samples[idx]['caption']
        improvement_val = sample_improvements[idx]
        print(f"  {i}. {caption}")
        print(f"     æ”¹è¿›: {improvement_val:+.6f} (æ–°:{new_similarities[idx]:.6f} vs æ—§:{old_similarities[idx]:.6f})")
    
    print(f"\nâš ï¸ æ”¹è¿›æœ€å°çš„5ä¸ªæ ·æœ¬:")
    for i, idx in enumerate(worst_improvements, 1):
        caption = unique_samples[idx]['caption']
        improvement_val = sample_improvements[idx]
        print(f"  {i}. {caption}")
        print(f"     æ”¹è¿›: {improvement_val:+.6f} (æ–°:{new_similarities[idx]:.6f} vs æ—§:{old_similarities[idx]:.6f})")
    
    # ä¿å­˜ç»“æœ
    os.makedirs(f"{output_dir}/fair_comparison", exist_ok=True)
    
    # ä¿å­˜æ ·æœ¬ä¿¡æ¯
    with open(f"{output_dir}/fair_comparison/{keyword}_samples.txt", 'w', encoding='utf-8') as f:
        for i, sample in enumerate(unique_samples):
            f.write(f"{i}: {sample['caption']}\n")
    
    # ä¿å­˜embeddings
    np.save(f"{output_dir}/fair_comparison/{keyword}_new_motion_embeddings.npy", new_motion_embeddings)
    np.save(f"{output_dir}/fair_comparison/{keyword}_new_text_embeddings.npy", new_text_embeddings)
    np.save(f"{output_dir}/fair_comparison/{keyword}_old_motion_embeddings.npy", old_motion_embeddings)
    np.save(f"{output_dir}/fair_comparison/{keyword}_old_text_embeddings.npy", old_text_embeddings)
    np.save(f"{output_dir}/fair_comparison/{keyword}_new_similarities.npy", new_similarities)
    np.save(f"{output_dir}/fair_comparison/{keyword}_old_similarities.npy", old_similarities)
    
    # ä¿å­˜å¯¹æ¯”ç»“æœ
    comparison_results = {
        'keyword': keyword,
        'n_samples': len(unique_samples),
        'new_model_path': new_model_path,
        'old_model_path': old_model_path,
        'new_mean_similarity': float(new_similarities.mean()),
        'old_mean_similarity': float(old_similarities.mean()),
        'improvement': float(improvement),
        'improvement_percent': float(improvement/old_similarities.mean()*100),
        'new_high_quality_ratio': float(new_high),
        'old_high_quality_ratio': float(old_high),
        'high_quality_improvement': float(high_quality_improvement),
        'new_std': float(new_similarities.std()),
        'old_std': float(old_similarities.std())
    }
    
    import json
    with open(f"{output_dir}/fair_comparison/{keyword}_comparison_results.json", 'w') as f:
        json.dump(comparison_results, f, indent=2)
    
    print(f"\nğŸ’¾ å…¬å¹³å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {output_dir}/fair_comparison/")
    
    # æ€»ç»“
    if improvement > 0:
        if improvement > 0.05:
            performance_level = "æ˜¾è‘—æå‡"
            emoji = "ğŸš€"
        elif improvement > 0.02:
            performance_level = "æ˜æ˜¾æå‡"
            emoji = "ğŸ“ˆ"
        else:
            performance_level = "å°å¹…æå‡"
            emoji = "â¬†ï¸"
    else:
        performance_level = "éœ€è¦è°ƒä¼˜"
        emoji = "âš ï¸"
    
    print(f"\n{emoji} å…¬å¹³å¯¹æ¯”æ€»ç»“: {performance_level}")
    print(f"åœ¨{len(unique_samples)}ä¸ªç›¸åŒæ ·æœ¬ä¸Šï¼Œæ–°æ¨¡å‹ç›¸æ¯”åŸå§‹æ¨¡å‹æœ‰{improvement/old_similarities.mean()*100:+.1f}%çš„æ”¹è¿›")
    
    return comparison_results, unique_samples, new_similarities, old_similarities


def encode_samples_with_model(samples, model_path, opt):
    """
    ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç¼–ç æ ·æœ¬åˆ—è¡¨
    """
    # åˆå§‹åŒ–æ¨¡å‹
    _init_clip_motion_model(model_path)
    model = GLOBAL_CACHE["clip_motion_align_model"]
    tokenizer = GLOBAL_CACHE["clip_tokenizer"]
    
    motion_embeddings = []
    text_embeddings = []
    
    # æŒ‰batchå¤„ç†æ ·æœ¬
    for i in tqdm(range(0, len(samples), opt.batch_size), desc="ç¼–ç æ ·æœ¬"):
        batch_samples = samples[i:i+opt.batch_size]
        
        # å‡†å¤‡æ–‡æœ¬
        captions = [sample['caption'].lower() for sample in batch_samples]
        text_enc = tokenizer(
            captions,
            padding=True,
            truncation=True,
            max_length=opt.max_length,
            return_tensors="pt"
        )
        input_ids = text_enc["input_ids"].to(opt.device)
        attention_mask = text_enc["attention_mask"].to(opt.device)
        
        # å‡†å¤‡motionæ•°æ®
        batch_motions = []
        batch_lengths = []
        for sample in batch_samples:
            motion = sample['motion']
            if not isinstance(motion, torch.Tensor):
                motion = torch.tensor(motion, dtype=torch.float32)
            else:
                motion = motion.float()  # ç¡®ä¿æ˜¯floatç±»å‹
            batch_motions.append(motion)
            batch_lengths.append(sample['length'])
        
        # Padding motions to same length
        max_len = max(m.shape[0] for m in batch_motions)
        padded_motions = []
        for motion in batch_motions:
            if motion.shape[0] < max_len:
                pad = torch.zeros((max_len - motion.shape[0], motion.shape[1]), dtype=torch.float32)
                motion = torch.cat([motion, pad], dim=0)
            padded_motions.append(motion)
        
        motion_tensor = torch.stack(padded_motions, dim=0).to(opt.device)
        length_tensor = torch.tensor(batch_lengths, dtype=torch.long, device=opt.device)
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            motion_emb, text_emb = model(motion_tensor, length_tensor, input_ids, attention_mask)
        
        motion_embeddings.append(motion_emb.cpu().numpy())
        text_embeddings.append(text_emb.cpu().numpy())
        
        # æ¸…ç†å†…å­˜
        torch.cuda.empty_cache()
    
    # åˆå¹¶ç»“æœ
    motion_embeddings = np.concatenate(motion_embeddings, axis=0)
    text_embeddings = np.concatenate(text_embeddings, axis=0)
    
    return motion_embeddings, text_embeddings


if __name__ == "__main__":
    # ä½¿ç”¨å…¬å¹³å¯¹æ¯”é‡æ–°è¯„ä¼°æ–°è®­ç»ƒçš„æ¨¡å‹
    print("=== ğŸ¯ ä½¿ç”¨å…¬å¹³å¯¹æ¯”æ–¹æ³•é‡æ–°è¯„ä¼°MoCLIPæ¨¡å‹ ===")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    import os
    os.makedirs("./new_model_results", exist_ok=True)
    
    # è¿›è¡Œå…¬å¹³å¯¹æ¯”
    results = fair_model_comparison(
        keyword="chicken",
        new_model_path="./checkpoints/moclip_training/best_model.pt",
        old_model_path="/data/wenshuo/project/jhz/StableMoFusion/data/clip_motion_align_epoch_21.pt",
        dataset_name='t2m',
        output_dir="./new_model_results",
        batch_size=32
    )
    
    if results:
        comparison_results, unique_samples, new_similarities, old_similarities = results
        print(f"\n=== ğŸ‰ å…¬å¹³å¯¹æ¯”ä»»åŠ¡å®Œæˆ ===")
        print(f"ä½¿ç”¨å®Œå…¨ç›¸åŒçš„{len(unique_samples)}ä¸ªæ ·æœ¬è¿›è¡Œå¯¹æ¯”")
        print(f"å¹³å‡ç›¸ä¼¼åº¦ä» {comparison_results['old_mean_similarity']:.6f} æå‡åˆ° {comparison_results['new_mean_similarity']:.6f}")
        print(f"çœŸå®æ”¹è¿›: {comparison_results['improvement_percent']:+.2f}%")
        print(f"é«˜è´¨é‡æ ·æœ¬æ¯”ä¾‹æ”¹è¿›: {comparison_results['high_quality_improvement']:.1%}")
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        print(f"\n=== ğŸ“ˆ ç»Ÿè®¡åˆ†æ ===")
        from scipy import stats
        
        # é…å¯¹tæ£€éªŒ
        t_stat, p_value = stats.ttest_rel(new_similarities, old_similarities)
        print(f"é…å¯¹tæ£€éªŒ: t = {t_stat:.4f}, p = {p_value:.6f}")
        if p_value < 0.001:
            significance = "ææ˜¾è‘— (p < 0.001) ***"
        elif p_value < 0.01:
            significance = "é«˜æ˜¾è‘— (p < 0.01) **"
        elif p_value < 0.05:
            significance = "æ˜¾è‘— (p < 0.05) *"
        else:
            significance = "ä¸æ˜¾è‘— (p â‰¥ 0.05)"
        print(f"ç»Ÿè®¡æ˜¾è‘—æ€§: {significance}")
        
        # æ•ˆåº”é‡ (Cohen's d)
        pooled_std = np.sqrt(((len(new_similarities)-1)*new_similarities.var() + 
                             (len(old_similarities)-1)*old_similarities.var()) / 
                            (len(new_similarities) + len(old_similarities) - 2))
        cohens_d = (new_similarities.mean() - old_similarities.mean()) / pooled_std
        print(f"æ•ˆåº”é‡ (Cohen's d): {cohens_d:.4f}")
        
        if abs(cohens_d) >= 0.8:
            effect_size = "å¤§æ•ˆåº”"
        elif abs(cohens_d) >= 0.5:
            effect_size = "ä¸­ç­‰æ•ˆåº”"
        elif abs(cohens_d) >= 0.2:
            effect_size = "å°æ•ˆåº”"
        else:
            effect_size = "å¾®å°æ•ˆåº”"
        print(f"æ•ˆåº”å¤§å°: {effect_size}")
        
        print(f"\nâœ… å…¬å¹³å¯¹æ¯”è¯å®ï¼šæ–°è®­ç»ƒçš„MoCLIPæ¨¡å‹ç¡®å®å–å¾—äº†çœŸå®çš„æ€§èƒ½æå‡ï¼")
    else:
        print("âŒ å…¬å¹³å¯¹æ¯”å¤±è´¥")